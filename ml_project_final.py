# -*- coding: utf-8 -*-
"""ML PROJECT FINAL

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/16HfRGcwOf_devF1QGqLIV4Ki7650FTxW

BOSTON HOUSE TAX PREDICTION

PROBLEM STATEMENT:
The problem that we are going to solve here is that given a set of features that describe a house in Boston, our machine learning model must predict the house price.
To train our machine learning model with boston housing data, we will be using scikit-learnâ€™s boston dataset.

In this dataset, each row describes a boston town or suburb. There are 506 rows and 13 attributes (features) with a target column (price).
The Boston Housing Dataset

The Boston Housing Dataset is a derived from information collected by the U.S. Census Service concerning housing in the area of Boston MA.
The following describes the dataset columns:

CRIM - per capita crime rate by town

ZN - proportion of residential land zoned for lots over 25,000 sq.ft.

INDUS - proportion of non-retail business acres per town.

CHAS - Charles River dummy variable (1 if tract bounds river; 0 otherwise)

NOX - nitric oxides concentration (parts per 10 million)

RM - average number of rooms per dwelling

AGE - proportion of owner-occupied units built prior to 1940

DIS - weighted distances to five Boston employment centres

RAD - index of accessibility to radial highways

TAX - full-value property-tax rate per $10,000

PTRATIO - pupil-teacher ratio by town

B - 1000(Bk - 0.63)^2 where Bk is the proportion of blacks by town

LSTAT - % lower status of the population

MEDV - Median value of owner-occupied homes in $1000's
"""

# Importing the libraries

# Utility Libraries

import io
import numpy as np
import pandas as pd

# Visualisation Libraries
import matplotlib.pyplot as plt
import seaborn as sns

# Data Processing Libraries
from sklearn.model_selection import train_test_split
from sklearn.model_selection import GridSearchCV
from sklearn.metrics import explained_variance_score, mean_absolute_error, mean_squared_error, r2_score
from sklearn.metrics import r2_score

# Algorithm Libraries
from sklearn.tree import DecisionTreeRegressor, plot_tree
from sklearn.linear_model import LinearRegression
from sklearn.ensemble import RandomForestRegressor

#importing csv file
from google.colab import files
uploaded=files.upload()
column_names = ['CRIM', 'ZN', 'INDUS', 'CHAS', 'NOX', 'RM', 'AGE', 'DIS', 'RAD', 'TAX', 'PTRATIO', 'B', 'LSTAT', 'MEDV']
df=pd.read_csv(io.BytesIO(uploaded['housing.csv']),header=None, delimiter=r"\s+",names=column_names)

#displaying csv file
print(df)
print()

#about dataset
print("SIZE OF DATA:")
print(df.size)

#Displaying info of dataset
print("DATASET INFO:")
df.info()
print()

#shape of dataset
print("SHAPE OF DATASET:")
df.shape

"""DATA VISUALIZATION"""

#Data visualization
fig=plt.figure()
ax=fig.add_subplot(1, 1, 1)
ax.hist(df['RM'], bins = 35)
plt.title("Average number of rooms Distribution ")
plt.xlabel("RM")
plt.ylabel("frequency")
plt.show()

fig=plt.figure()
ax=fig.add_subplot(1, 1, 1)
ax.hist(df['LSTAT'], bins = 35)
plt.title("Homeowners distribution with low class")
plt.xlabel("LSTAT")
plt.ylabel("frequency")
plt.show()

fig=plt.figure()
ax=fig.add_subplot(1, 1, 1)
ax.hist(df['PTRATIO'], bins = 35)
plt.title("Students to Teachers ratio distribution")
plt.xlabel("PTRATIO")
plt.ylabel("frequency")
plt.show()

fig=plt.figure()
ax=fig.add_subplot(1, 1, 1)
ax.hist(df['AGE'], bins = 35)
plt.title("Ages of Black Owned Portion in the town")
plt.xlabel("AGE")
plt.ylabel("B")
plt.show()

#correlation of target field with other features
df.corr()['TAX'].sort_values(ascending=False)

#scaling down everything to same scale[0-1]
from sklearn.preprocessing import StandardScaler
X = df.drop('TAX',axis=1)
y = df['TAX']
scaler = StandardScaler()
X = scaler.fit_transform(X)

#splitting data to 70-30 proportions
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)

"""TAX PREDICTION USING MULTIPLE LINEAR REGRESSION"""

#Training house tax prediction model using multiple linear regression
model = LinearRegression()
model.fit(X_train, y_train)

# Predict the target variable for the testing data
y_pred = model.predict(X_test)

# Evaluate the model using R-squared score
score = r2_score(y_test, y_pred)

print()

#watching the results
res = pd.DataFrame()
res['Actual tax'] = y_test
res['Predicted tax'] =y_pred
res.head()
print(res)
print()

# Evaluate model performance
score = r2_score(y_test, y_pred)
expl_tr = explained_variance_score(y_test, y_pred)
mae = mean_absolute_error(y_test, y_pred)
mse = mean_squared_error(y_test, y_pred)
rmse = np.sqrt(mse)

# Print model performance metrics
print("Multiple Linear Regression Model Score : ",round(score * 100, 2) )
print("Explained Variance Score               : ", expl_tr)
print("Mean Absolute Error                    : ", mae)
print("Mean Squared Error                     : ", mse)
print("Root Mean Squared Error                : ", rmse)
print("R-squared                              : ", score)
print()
accuracy = round(model.score(X_test, y_test) * 100)
print("Multiple Linear Regression Model Accuracy is", accuracy, "%")
print()

# Plot predicted vs actual values
plt.scatter(y_test, y_pred, alpha=0.5)
plt.xlabel("Actual Values")
plt.ylabel("Predicted Values")
plt.title("Multiple Regression Model")
plt.show()

"""
TAX PREDICTION USING DECISION TREE"""

#Training house tax prediction model using decision tree
tr_regressor = DecisionTreeRegressor(random_state=0)
tr_regressor.fit(X_train, y_train)

# Make predictions on test data
y_pred = tr_regressor.predict(X_test)


#watching the results
res = pd.DataFrame()
res['Actual tax'] = y_test
res['Predicted tax'] = y_pred
res.head()
print(res)
print()

# Evaluate model performance
decision_score = tr_regressor.score(X_test, y_test)
expl_tr = explained_variance_score(y_test, y_pred)
mae = mean_absolute_error(y_test, y_pred)
mse = mean_squared_error(y_test, y_pred)
rmse = np.sqrt(mse)
r2 = r2_score(y_test, y_pred)

# Print model performance metrics
print("Decision tree Regression Model Score : ", round(decision_score*100))
print("Explained Variance Score             : ", expl_tr)
print("Mean Absolute Error                  : ", mae)
print("Mean Squared Error                   : ", mse)
print("Root Mean Squared Error              : ", rmse)
print("R-squared                            : ", r2)
print()
accuracy = round(tr_regressor.score(X_test, y_test) * 100)
print("Decision tree Regression Model Accuracy is", accuracy, "%")
print()

# Plot decision tree
plt.figure(figsize=(20,10))
plot_tree(tr_regressor)
plt.show()

"""TAX PREDICTION USING RANDOM FOREST"""

#Training house tax prediction model using random forest
rfc = RandomForestRegressor()
params = {'n_estimators':[100,200,300,400,500,600,700,800,900,1000]}
grid_model = GridSearchCV(rfc, params,verbose=2)
grid_model.fit(X_train,y_train)
pred = grid_model.predict(X_test)

#checking the best parameter for house tax prediction
grid_model.best_params_

#watching the results
res = pd.DataFrame()
res['Actual tax'] = y_test
res['Predicted tax'] = pred
res.head()

# Evaluate model performance
expl_tr = explained_variance_score(y_test, pred)
mae = mean_absolute_error(y_test, pred)
mse = mean_squared_error(y_test, pred)
rmse = np.sqrt(mse)
r2 = r2_score(y_test, pred)
print()

# Print model performance metrics
print("Random Forest Model Score            : ", round(r2*100))
print("Explained Variance Score             : ", expl_tr)
print("Mean Absolute Error                  : ", mae)
print("Mean Squared Error                   : ", mse)
print("Root Mean Squared Error              : ", rmse)
print("R-squared                            : ", r2)
print()

print("Random Forest Model Accuracy is", r2*100, "%")
print()

#plotting the results
sns.scatterplot(x=y_test , y=pred ,alpha=0.8)
plt.xlabel('Real values')
plt.ylabel('Predicted values')

#checking the best parameter for house tax prediction
grid_model.best_params_

"""In this project we have used three Machine Learning Algorithms to predict the house tax:

1. Random Forests : Random forest is a commonly-used machine learning algorithm which combines the output of multiple decision trees to reach a single result. Its ease of use and flexibility have fueled its adoption, as it handles both classification and regression problems.

2. Multiple Regression : Multiple linear regression is a statistical technique that uses multiple linear regression to model more complex relationships between two or more independent variables and one dependent variable.

3. Decision Trees : A decision tree is a type of supervised machine learning used to categorize or make predictions based on how a previous set of questions were answered. The model is a form of supervised learning, meaning that the model is trained and tested on a set of data that contains the desired categorization.

The dataset that we have used consists of the following features which affects the tax prediction as given below:

1. CRIM - A higher crime rate in a town may lead to lower property values, and thus a lower tax rate. Conversely, a low crime rate in a town may lead to higher property values and a higher tax rate.

2. ZN - A higher proportion of residential land zoned for lots over 25,000 sq.ft may indicate that the town has higher property values, which may lead to a higher tax rate.

3. INDUS - A higher proportion of non-retail business acres per town may indicate that the town is more industrialized, which may lead to a higher tax rate.

4. CHAS - The Charles River dummy variable indicates whether a tract of land borders the river or not. If a town is located near the river, it may have higher property values and a higher tax rate.

5. NOX - Higher levels of nitric oxides concentration may indicate higher levels of pollution and could lead to lower property values and a lower tax rate.

6. RM - A higher average number of rooms per dwelling may indicate larger, more expensive homes, which may lead to a higher tax rate.

7. AGE - A higher proportion of owner-occupied units built prior to 1940 may indicate older homes, which may have lower property values and a lower tax rate.

8. DIS - Weighted distances to five Boston employment centres may indicate a more central location, which may lead to higher property values and a higher tax rate.

9. RAD - A higher index of accessibility to radial highways may indicate a more central location and easier access to amenities, which may lead to higher property values and a higher tax rate.

10. TAX - Full-value property-tax rate per $10,000. This is the target variable and is predicted based on the other features.

11. PTRATIO - A higher pupil-teacher ratio may indicate lower quality schools, which may lead to lower property values and a lower tax rate.

12. B - The proportion of blacks by town may be a proxy for social and economic factors that may influence property values and tax rates.

13. LSTAT - A higher percentage of lower status of the population may indicate lower property values and a lower tax rate.

14. From all the above implemented algorithms , the accuracy of Random Forest Algorithm is found to be the greatest followed by Decision Tree and Multiple Regression.

Therefore the Accuracy can be summarized as follows:

Model accuracy of multiple linear regression is : 89%

Model accuracy of decision tree is : 95%

Model accuracy of random forest is : 98%

Random Forest > Decision Tree > Multiple Regression


Hence Random Forest Algorithm provides us the greatest accuracy in predicting the House Tax in comparison to Decision Tree and Multiple Regression
"""